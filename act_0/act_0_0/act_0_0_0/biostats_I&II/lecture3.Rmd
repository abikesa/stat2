---
title: "Lecture 3 in Biostatistics 651-652 at JHU Bloomberg School of Public Health"
author: "Ciprian M. Crainiceanu"
date: "Monday, August 08, 2016"
output: html_document
---

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>. RStudio is the recommended software for using the document.  

This document is used together with the Lectures notes for Lecture 3 in the Biostatistics 651-652 series. The idea is to learn how to produce reproducible Biostatistical documents where text, data, and results are interweaved. R is taught by reading and understanding these documents, but learning R requires much additional individual effort.

Let us play some games and see whether we can uncover some order from randomness. Simulate the mean of $n$ die rolls, where $n$ varies from $5$, $10$, $20$, $100$ 

```{r}
mx5 <- rep(0, 1000)                              #store the means for n=5
mx10=mx5                                         #store the means for n=10
mx20=mx5                                         #store the means for n=20
mx100=mx5                                        #store the means for n=100
for ( i in 1:1000 )
  {#begin rolling the die
  mx5[i] <- mean(sample(1:6,5,replace=T))        #roll the die n=5 times and take the mean
  mx10[i] <- mean(sample(1:6,10,replace=T))
  mx20[i] <- mean(sample(1:6,20,replace=T))
  mx100[i] <- mean(sample(1:6,100,replace=T))    #roll the die n=100 times and take the mean
  }#end rolling the die
```

Now plot the results in a Figure with 4 subfigures. Each subplot contains the histogram of $1000$ realizations of the mean of $n$ die rolls, where $n$ varies. Each histogram can be viewed as the empirical distribution of the mean over a fixed number of samples, $n$.

```{r}
par(mfrow = c(2, 2))
hist(mx5,breaks=20,probability=TRUE,xlab="Mean in 5 die rolls",xlim=c(1,6),cex.lab=1.5,cex.axis=1.5,col.axis="blue",main=NULL)
hist(mx10,breaks=20,probability=TRUE,xlab="Mean in 10 die rolls",xlim=c(1,6),cex.lab=1.5,cex.axis=1.5,col.axis="blue",main=NULL)
hist(mx20,breaks=20,probability=TRUE,xlab="Mean in 20 die rolls",xlim=c(1,6),cex.lab=1.5,cex.axis=1.5,col.axis="blue",main=NULL)
hist(mx100,breaks=20,probability=TRUE,xlab="Mean in 100 die rolls",xlim=c(1,6),cex.lab=1.5,cex.axis=1.5,col.axis="blue",main=NULL)
```

Try to look at the Figure and identify the main characteristics of the plots and what changes as the number of die rolls increases. What do you expect the empirical means to be around? Are the empirical means exactly equal to the theoretical means?

Mean of various distributions
```{r}
par(mfrow = c(2, 2))
x1=seq(-3.5,3.5,length=101)
y1<-dnorm(x1)
plot(x1,y1,type="l",col="blue",lwd=3,xlab="Error",ylab="Density",cex.lab=1.5,cex.axis=1.5,col.axis="blue")
lines(c(0,0),c(0,0.4),col="red",lwd=3)

x2=seq(0,15,length=101)
y2<-dexp(x2,1/5)
plot(x2,y2,type="l",col="blue",lwd=3,xlab="Surviving time",ylab="Density",cex.lab=1.5,cex.axis=1.5,col.axis="blue")
lines(c(5,5),c(0,0.2),col="red",lwd=3)

x3=seq(0,40,length=101)
y3<-dgamma(x3,shape=2,scale=5)
plot(x3,y3,type="l",col="blue",lwd=3,xlab="Knee response time",ylab="Density",cex.lab=1.5,cex.axis=1.5,col.axis="blue")
lines(c(10,10),c(0,0.073),col="red",lwd=3)

x4=seq(0,1,length=101)
y4<-dbeta(x4,shape1=10,shape2=2)
plot(x4,y4,type="l",col="blue",lwd=3,xlab="Probability of response",ylab="Density",cex.lab=1.5,cex.axis=1.5,col.axis="blue")
lines(c(0.83,0.83),c(0,4.2),col="red",lwd=3)
```

Note that the mean does not need to be the middle of the variable domain, or the point of maximum probability (likelihood), but it could be for symmetric unimodal distributions, such as the Gaussian (normal) or Laplace (double-exponential).

We now show the effect of mean and variance on the shape of the Normal (first row subplots) Gamma (second row subplots) distributions

```{r}
x=seq(-10,10,length=201)
par(mfrow = c(2, 2))
y1<-dnorm(x)
y2<-dnorm(x,0,2)
y3<-dnorm(x,0,4)
plot(x,y1,type="l",col="blue",lwd=3,xlab="Error",ylab="Density",cex.lab=1.5,cex.axis=1.5,col.axis="blue",main="Same mean N")
lines(x,y2,col="orange",lwd=3)
lines(x,y3,col="red",lwd=3)

y1<-dnorm(x,4,1)
y2<-dnorm(x,2,2)
y3<-dnorm(x,0,4)
plot(x,y1,type="l",col="blue",lwd=3,xlab="Error",ylab="Density",cex.lab=1.5,cex.axis=1.5,col.axis="blue",main="Different mean/variance N")
lines(x,y2,col="orange",lwd=3)
lines(x,y3,col="red",lwd=3)

x=seq(0,80,length=101)
y1<-dgamma(x,shape=10,scale=2)
y2<-dgamma(x,shape=5,scale=4)
y3<-dgamma(x,shape=2,scale=10)
plot(x,y1,type="l",col="blue",lwd=3,xlab="Surviving time",ylab="Density",cex.lab=1.5,cex.axis=1.5,col.axis="blue",main="Same mean G")
lines(x,y2,col="orange",lwd=3)
lines(x,y3,col="red",lwd=3)

x=seq(0,80,length=101)
y1<-dgamma(x,shape=10,scale=sqrt(10))
y2<-dgamma(x,shape=5,scale=sqrt(20))
y3<-dgamma(x ,shape=2,scale=sqrt(50))
plot(x,y1,type="l",col="blue",lwd=3,ylim=c(0,0.06),xlab="Surviving time",ylab="Density",cex.lab=1.5,cex.axis=1.5,col.axis="blue",main="Same variance G")
lines(x,y2,col="orange",lwd=3)
lines(x,y3,col="red",lwd=3)
```

Calculating the empirical variance and standard deviation in `R`

```{R}
y<-rnorm(100,0,4)
round(var(y),digits=3)
round(sd(y),digits=3)
```

Calculate the sample variance for the toss of a die. From Lecture 3, it follows that if $X$ is the random variable describing the roll of a die then $Var(X)=\frac{1}{6}\sum_{k=1}^6  k^2-(\frac{1}{6}\sum_{k=1}^6 k)^2$. This can be calculated as follows

```{r}
x=1:6                                       #Vector of possible values for a 1-die roll
ex2=sum(x^2*rep(1/6,6))                     #Expected value of X^2                     
ex=sum(x*rep(1/6,6))                        #Expected value of X
varx=ex2-ex^2                               #Variance of X
round(varx,digits=3)
```

Comparing Chebishev with parametric assumptions (See Table under same title in Lecture 3) in terms of probability of exceeding $k\sigma$. Recall that Cebishev's inequality states that for a random variable with mean $\mu$ and standard deviation $\sigma$ we have 
$$P(|X-\mu|>k\sigma)<\frac{1}{k^2}.$$
This type of inequality is very useful in Statistics as it can be re-written as
$$P(X-k\sigma<\mu<X+k\sigma)>1-\frac{1}{k^2},$$
which indicates that the *the true mean of the variable X lies between $k\sigma$ of the observation X with probability at least $1-1/k^2$.* This is extremely useful for building confidence intervals for the mean of a random variable. For example, for $k=4.47$ there is at least a 95\% chance that the interval $(X-4.47\sigma,X+4.47\sigma)$ covers the true mean $\mu$. 

This interval is typically very conservative and some knowledge of the distribution form of $X$ can substantially reduce the length of the confidence interval. FOr example, if we know that the distribution of $X$ is Normal then we know that there is a 95\% chance that the interval $(X-1.96\sigma,X+1.96\sigma)$ covers the true mean $\mu$. This interval has a length that is $43$\% of the length of the Cebishev confidence interval for the same level of confidence ($95$\%). Thus, having information about the shape of the distribution can dramatically reduce the length of the confidence intervals (and p-values; more about this later.)

```{r}
k=2:5                                            #Multiples of SD
round(1/k^2,digits=3)                            #Chebisev upper limit 
2*(1-pnorm(k))                                   #Probability for a Normal distribution
sdt3=sqrt(3)                                     #SD of a t(3) distribution
round(2*(1-pt(k*sdt3,df=3)),digits=3)            #Probability for a t(3) distribution
sh=2                                             #Shape parameter of Gamma(sh,sc)
sc=2                                             #Scale parameter of Gamma(sh,sc)
m=sh*sc                                          #Mean of Gamma(sh,sc)
sdg = sqrt(sh*sc^2)                              #SD of Gamma(sh,sc)
pgam=pgamma(m-k*sdg,shape=sh,scale=sc)+1-pgamma(m+k*sdg,shape=sh,scale=sc)  
round(pgam,digits=3)                             #Probability for a Gamma(2,2) distribution 
```

Note that the probability of exceeding a certain threshold $k\sigma$ is much larger using the Cebishev upper bound, as the inequality holds for any random variable, including random variables with very heavy tails and heavy skew. While the $t(3)$ distribution is heavy-tailed, there are other distributions that have even heavier tails. The Normal distribution tends to have the smaller probabilities of exceeding a particular multiple of the standard deviation, though for $2$ standard deviations it does not seem to matter that much whether one uses the Normal, Gamma(2,2) or $t(3)$ distribution. Differences become much more pronounced for higher values of the multiplier, $k$. This is due to the much thinner tail of the normal (the tail is dominated by $exp(-x^2/2)$), which converges very quickly to zero. Distributions with slower tail decays than the Normal, such as t(3) and Gamma(2,2), are called heavy-tail distributions.